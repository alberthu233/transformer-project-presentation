{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506a1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbef3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/workspace/ligo_general/prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fb84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_length, max_target_length):\n",
    "        self.original_text = data['original_text']\n",
    "        self.rewritten_text = data['rewritten_text']\n",
    "        self.prompt_text = data['rewrite_prompt']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.input_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.labels = []\n",
    "        for i in range(len(self.original_text)):\n",
    "            original_text = self.original_text[i]\n",
    "            rewritten_text = self.rewritten_text[i]\n",
    "            prompt_text = str(self.prompt_text[i])\n",
    "            task_prefix = f\"Recover instruction for the text transformation: original text: {original_text}, transformed text: {rewritten_text}\"\n",
    "            encoding =  self.tokenizer(\n",
    "                task_prefix,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_source_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "            target_encoding = self.tokenizer(\n",
    "                prompt_text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_target_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            labels = target_encoding.input_ids\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            self.input_ids.append(input_ids.squeeze())\n",
    "            self.attention_mask.append(attention_mask.squeeze())\n",
    "            self.labels.append(labels.squeeze())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }\n",
    "\n",
    "class PromptDecoderLightning(pl.LightningModule):\n",
    "    def __init__(self, model, tokenizer, max_source_length, max_target_length):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        loss = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        loss = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=self.max_target_length)\n",
    "        predicted_text = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return predicted_text\n",
    "    \n",
    "    def generate(self, original_text, rewritten_text):\n",
    "        task_prefix = f\"Recover instruction for text transformation: original text: {original_text}, transformed text: {rewritten_text}\"\n",
    "        encoding = self.tokenizer(\n",
    "            task_prefix,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_source_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=self.max_target_length)\n",
    "        predicted_text = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return predicted_text\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480d398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-large')\n",
    "\n",
    "# Define the maximum source and target lengths\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "#model = PromptDecoderLightning(model, tokenizer, max_source_length, max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3466a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "#checkpoint = 'llm-prompt-recovery/ckpt/flant5-te-epoch=09-val_loss=0.44.ckpt'\n",
    "checkpoint = 'ckpt/flant5-encoder-epoch=09-val_loss=0.44.ckpt'\n",
    "model = PromptDecoderLightning.load_from_checkpoint(checkpoint, model=model, tokenizer=tokenizer, max_source_length=512, max_target_length = 128, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6464a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/mistral_df.csv')\n",
    "# select the random 50 samples from row = 150 - 1500\n",
    "df = df.iloc[:2000].sample(100, random_state=0)\n",
    "# trim the dataset so each column has a maximum of 256 characters\n",
    "df['original_text'] = df['original_text'].str.slice(0, 512)\n",
    "df['rewritten_text'] = df['rewritten_text'].str.slice(0, 512)\n",
    "\n",
    "df = df.reset_index()\n",
    "dataset = PromptDataset(df, tokenizer, max_source_length, max_target_length)\n",
    "val_loader = DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b24bf56",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 39.39 GiB total capacity; 13.58 GiB already allocated; 158.44 MiB free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     predicted_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1609\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1603\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1604\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1605\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1609\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1625\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:3141\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   3135\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3136\u001b[0m     outputs,\n\u001b[1;32m   3137\u001b[0m     model_kwargs,\n\u001b[1;32m   3138\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3139\u001b[0m )\n\u001b[1;32m   3140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3141\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temporary_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_key_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   3146\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2800\u001b[0m, in \u001b[0;36mGenerationMixin._temporary_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;66;03m# Exception 1: code path for models using the legacy cache format\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m-> 2800\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;66;03m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;66;03m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbloom\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_class \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptbigcode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_class:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1858\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration._reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1854\u001b[0m reordered_layer_past_states \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past_state \u001b[38;5;129;01min\u001b[39;00m layer_past_states:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m     reordered_layer_past_states \u001b[38;5;241m=\u001b[39m reordered_layer_past_states \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m-> 1858\u001b[0m         \u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1859\u001b[0m     )\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreordered_layer_past_states[0] shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and layer_past_states[0] shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mismatched\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1864\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 39.39 GiB total capacity; 13.58 GiB already allocated; 158.44 MiB free; 13.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "model.eval()\n",
    "for i, batch in enumerate(val_loader):\n",
    "    outputs = model.model.generate(batch['input_ids'].to(device), max_new_tokens=64, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    predicted_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    mask = batch['labels'] != -100\n",
    "\n",
    "    # Apply the mask to the labels\n",
    "    labels = [label[mask[i]] for i, label in enumerate(batch['labels'])]\n",
    "\n",
    "    # Decode each sequence separately\n",
    "    labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "\n",
    "    predictions.extend(predicted_text)\n",
    "    ground_truth.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpened_cosine_similarity(s, k, q=1e-6, p=3):\n",
    "    \"\"\"\n",
    "    Calculates the sharpened cosine similarity between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    s (numpy.ndarray): The first vector.\n",
    "    k (numpy.ndarray): The second vector.\n",
    "    q (float, optional): A small constant to avoid division by zero. Default is 1e-6.\n",
    "    p (int, optional): The power to which the sharpening term is raised. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    float: The average sharpened cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    assert s.shape == k.shape\n",
    "    cosine_sims = []\n",
    "    for s_vec, k_vec in zip(s, k):\n",
    "        cosine_sim = np.dot(s_vec, k_vec) / (np.linalg.norm(s_vec) * np.linalg.norm(k_vec))\n",
    "        cosine_sims.append(np.sign(cosine_sim) * np.power(np.abs(cosine_sim) / (np.linalg.norm(s_vec) + q), p))\n",
    "    return np.mean(np.array(cosine_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Original text:  we study a stochastic game where one player tries to find a strategy such that the state process reaches a target of controlled - loss - type , no matter which action is chosen by the other player .   we provide , in a general setup , a relaxed geometric \n",
      "--------\n",
      "Rewritten text:    **Relaxed Geometric Dynamic Programming Principle:**  In a stochastic game, find a strategy such that the state process reaches a target state, regardless of the actions chosen by the other player. This can be achieved by iteratively refining a sequence of relaxed targets, starting from the initial state and moving towards the target state, until the desired target state is reached.  **Main Ide\n",
      "--------\n",
      "Predicted: emphasizes the key concepts and ideas, while omitting unnecessary details and jargon. focus on the main idea of the text, which is to find a strategy to iteratively improve the game process.\n",
      "--------\n",
      "Ground truth: emphasizes the main idea of the text more clearly. for example, you could focus on the problem of finding a strategy to reach a target state in a stochastic game, or you could focus on the relaxed geometric dynamic programming principle.\n",
      "\n",
      "--------\n",
      "Original text: Eden Hazard is a doubt for Chelsea's clash with West ham on Boxing Day . Oscar will be hoping for a recall after being on the bench against Stoke . Mark Noble returns for the Hammers after four games out . James Tomkins will be Sam Allardyce's only absente\n",
      "--------\n",
      "Rewritten text:   The Blues are currently in third place in the league, while the Hammers are in fifth. A win for Chelsea would move them into second place, while a win for West Ham would move them into third place.  The match is expected to be a close one, with both teams having strong lineups. Chelsea will be hoping to get goals from their star players, such as Eden Hazard, Diego Costa, and Pedro. West Ham will\n",
      "--------\n",
      "Predicted: emphasizes the significance of the match and the potential impact of each goal.\n",
      "--------\n",
      "Ground truth: emphasizes the significance of the upcoming match between chelsea and west ham on boxing day.\n",
      "\n",
      "--------\n",
      "Original text:  two - field slow - roll inflation is the most conservative modification of a single - field model .   the main motivations to study it are its entropic mode and non - gaussianity .   several years ago , for a two - field model with additive separable pote\n",
      "--------\n",
      "Rewritten text:   The slow-roll inflation paradigm, characterized by a single scalar field rolling slowly down a potential, has been a dominant framework for understanding inflation in the universe. However, certain theoretical and observational challenges have led to the exploration of alternative inflation models. One such model is slow-roll inflation with two fields, which introduces additional degrees of free\n",
      "--------\n",
      "Predicted: rewrite the text above into a concise and clear summary, focusing on the main points and omitting unnecessary details.\n",
      "--------\n",
      "Ground truth: write a continuation of the text above, discussing the implications of the results for future inflation models and observational evidence.\n",
      "\n",
      "--------\n",
      "Original text: Hundreds of flights a day pass over territory held by likes of ISIS in Iraq . Major Europe-Asia flight path is directly above stronghold city of Mosul . Claims made it is 'perfectly possible' ISIS could have missiles to destroy jets . Scrutiny on flight pa\n",
      "--------\n",
      "Rewritten text:   The situation is particularly alarming given the proximity of the major Europe-Asia flight path to Mosul, a city that has been fiercely contested between ISIS and Iraqi forces. The potential for ISIS to launch attacks on passenger flights from its stronghold in Mosul raises serious concerns about the safety of passengers and crew.  While there have not been any reported incidents of ISIS targeti\n",
      "--------\n",
      "Predicted: emphasizes the potential for an attack by isis on passenger flights.\n",
      "--------\n",
      "Ground truth: emphasizes the potential threat posed by isis's control of airspace in iraq, and how it has raised concerns about the safety of passenger flights.\n",
      "\n",
      "--------\n",
      "Original text: Ernestine Shepherd, 77, is one of the world's oldest bodybuilders . Only started working out when she was 56 thanks to her sister . When her sister died, she got depression and high blood pressure . But thanks to the support of her family, she began traini\n",
      "--------\n",
      "Rewritten text:   Ernestine Shepherd, a woman of indomitable spirit and unwavering determination, has captivated the world with her extraordinary journey as a bodybuilder at the ripe old age of 77. Born into a world brimming with challenges, Shepherd's story is a testament to her resilience, strength, and unwavering ability to overcome adversity.  Growing up, Shepherd faced a myriad of obstacles. Born into a fami\n",
      "--------\n",
      "Predicted: rewrite the text above, focusing on the positive aspects of her story, rather than the negative aspects. highlight the challenges she faced as a young woman, such as her poor upbringing and her struggles with depression and high blood pressure. describe her motivations for starting training, and how she overcame\n",
      "--------\n",
      "Ground truth: emphasizes the positive aspects of ernestine shepherd's story, such as her resilience, strength, and determination. focus on her ability to overcome adversity and inspire others through her example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get 5 random int from 0 to 100\n",
    "ids = random.sample(range(100), 5)\n",
    "for i in ids:\n",
    "    print(\"--------\")\n",
    "    print(f\"Original text: {df['original_text'][i]}\")\n",
    "    print(\"--------\")\n",
    "    print(f\"Rewritten text: {df['rewritten_text'][i]}\")\n",
    "    print('--------')\n",
    "    print(f\"Predicted: {predictions[i]}\")\n",
    "    print('--------')\n",
    "    print(f\"Ground truth: {ground_truth[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return SentenceTransformer('sentence-transformers/sentence-t5-base', device='cpu')\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6594531449426563\n"
     ]
    }
   ],
   "source": [
    "predictions_embeddings = model.encode(predictions)\n",
    "ground_truth_embeddings = model.encode(ground_truth)\n",
    "\n",
    "scs = sharpened_cosine_similarity(predictions_embeddings, ground_truth_embeddings)\n",
    "print(scs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
